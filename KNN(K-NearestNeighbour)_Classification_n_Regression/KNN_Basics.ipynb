{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN (K Nearest Neighbour) Classification n Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is KNN?<br>\n",
    "\n",
    "KNN is technique which is used for Classification Problems for example Amazon Reviews to classify whether a review is positive or negative.<br>\n",
    "\n",
    "Its done by Training a function f(x) i.e we will train this function from the given dataset and after that if we give any queryPoint to this function it will return whether its positive or negative review.<br>\n",
    "\n",
    "### How KNN is calculated?<br>\n",
    "\n",
    "When we pass a querypoint(the point which need to be classified) to y=f(x), It will calculate 'K nearest Neighbours' to the querypoint so that we will get k yi's(i.e positive or negative) and if majority of Yi's are positive the Querypoint is considered as positive review or else negative review.<br>\n",
    "\n",
    "### Where KNN Fails?<br>\n",
    "\n",
    "KNN fails in 2 cases:<br>\n",
    "\n",
    "Case 1: When the query point is far away from both the positive and negative points(i.e its an outlier), then its hard to classify.<br>\n",
    "\n",
    "Case 2: When Data Points are randomly spread the its hard to classify if the querypoint is positive or negative review.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERMS Used in this concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean(L2) distance -> distance between 2 points regular formula.<br>\n",
    "\n",
    "Manhattan(L1) distance -> is sum of 2 sides other than Hypotenuse.<br>\n",
    "\n",
    "Minkowski -> generalized formula for L2 and L1.<br>\n",
    "\n",
    "Hamming Distance -> Its the count of no.of difference values Between 2 vectors<br>\n",
    "Example:<br>\n",
    "\n",
    "x -> [a,b,a,a,b]<br>\n",
    "y -> [a,a,b,a,b]<br>\n",
    "\n",
    "In the above case Hamming distance is 2 as in positions/indices 1,2 values are not same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Decision Surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/knn1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image the curve which is not smmoth is called Decision Surface,\n",
    "The smoothness of the curve depends on the 'K' Value i.e if K increases Smoothness Increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations:<br>\n",
    "\n",
    "If K=n: Example Amazon reviews positive/negative, If there are 50 positive points and 20 negative points and K=n then query point will be classified as Positive as there are more no.of pos points and we are considering K as 'n'.<br>\n",
    "\n",
    "It Even fails when no.of Positive points = no.of Negative points, i.e it cannot classify whether its a positive point or negative point.<br>\n",
    "\n",
    "Shown in the below image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/knn2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OverFitting and UnderFitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/knn3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverFitting:(K is min)<br>\n",
    "\n",
    "In this case we are considering every minute detail i.e considering every data point which is even an Outlier so that we can create a 'Perfect Curve', as we are considering noisy data the results may not be likely.\n",
    "\n",
    "\n",
    "### WellFit:( min < k < max )<br>\n",
    "\n",
    "In This Case the noisy data is not considered as K value is more than 1, So its the Curve which is Ideal to deal with, So we need to find this K value so that curve is Well Fit.\n",
    "\n",
    "\n",
    "### UnderFitting:(K is n or max)<br>\n",
    "Here If there are more no.of positive points than negative points then Query point is considered as Positive point, its the worst fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How To determine the right K Value?<br>\n",
    "\n",
    "Let 'D' be total data set,<br>\n",
    "Its divided into Three Parts:<br>\n",
    "1)D-Train ----------------------------> 70% of DataPoints<br> \n",
    "2)D-CV (Cross Validation) ------------> 10% of DataPoints<br>\n",
    "3)D-Test -----------------------------> 20% of DataPoints<br>\n",
    "\n",
    "Note: Division of Data is Our Wish<br>\n",
    "\n",
    "Now, Perform the operations below:<br>\n",
    "\n",
    "K = 1 --------> apply K-NN on D-Train -------> Check accuracy of this model \n",
    "                                               By passing\n",
    "                                               D-CV data points to this\n",
    "                                               trained model.<br>\n",
    "                                               \n",
    "\n",
    "K = 2 --------> apply K-NN on D-Train -------> Check accuracy of this model \n",
    "                                               By passing\n",
    "                                               D-CV data points to this\n",
    "                                               trained model.<br>\n",
    "                                               \n",
    ".<br>\n",
    ".<br>\n",
    ".<br>\n",
    "\n",
    "Until the accuracy starts decreasing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the above case We considered 70% datapoints as Train Data and rest 30 as DCV and DTest, But In ML if we have more data more Strong is our Model, so To consider More percent of Data as DTrain We use K-Fold Cross Validation.<br>\n",
    "\n",
    "Note: DCV and DTest will be considered as above case only but Dtrain Will change i.e Dtrain will be 80%, This can be achieved by performing below operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/knn4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/knn5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So By doing this we are not wasting our data as in the previous technique we have considered DTrain as 70% DCV as 10% and DTest as 20%, here DTrain is 80%, DCV is 80% and DTest is 20%. but DCV and DTrain Doesnt interfer i.e both are different.\n",
    "\n",
    "In The above case Dtrain, DTest, DCV all three's data points are different at any instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
